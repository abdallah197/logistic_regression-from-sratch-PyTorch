{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sEMLHKckbrpw"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "# **Logistic regression from scratch using pytorch**\n",
    "\n",
    "\n",
    "Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes.\n",
    "\n",
    "In this notebook, we explore the binary logistic regression. We change the clsssification problem with 10 classes into a binary classification problem by considering only the points from the classes *'ship'* and *'car'*.\n",
    "\n",
    "![Logistic regression image](https://drive.google.com/uc?id=1eRF1-2qnQYAkkCDpAwROj5MiMKzct2x0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NsvjieqtuRnx"
   },
   "source": [
    "** **bold text**Task 1. Generating training dataset** \n",
    "\n",
    "Since logistic regression is a classification problem with two classes, we need a labelled dataset with two classes as the training set. Complete the function to obtain the datapoints corresponding to labels *'ship'* and *'car'*. \n",
    "\n",
    "steps to build a new dataset with these coditions: \n",
    "\n",
    "1. Loading the train and test sets of CIFAR 10 from torchvision using a batch size of 1024.\n",
    "2. Splitting the training samples by 80:20 ratio into train set and validation set respectively. \n",
    "3. Filtering the datasets to only have images with classes 'ship' or 'car'. The corresponding labels are 8 and 1 respectively.\n",
    "4. The new labels for binary classification problem should be 'ship' : 0 and 'car': 1.\n",
    "5. Define a [torch.utils.data.Dataset](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html) with the filtered tensors of images and the newly created labels.\n",
    "6. Define a dataloader for training and validation datasets with batch_size 64.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MlZ6WVYBugBL"
   },
   "outputs": [],
   "source": [
    "# TODO : generate the train, validation, and test sets from CIFAR 10  \n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, utils, datasets\n",
    "from torch import nn,autograd\n",
    "from torch.autograd import Variable\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "v54PSq9Eq87-",
    "outputId": "8abbb5af-c64d-4332-a3fd-9f1fbdb251de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170500096it [00:06, 27175875.93it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar/cifar-10-python.tar.gz to data/cifar/\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #mean, std for Red, Green and Blue\n",
    "])\n",
    "train = datasets.CIFAR10(root='data/cifar/', train=True, transform=transform, target_transform=None, download=True)\n",
    "test =datasets.CIFAR10(root='data/cifar/', train=False, transform=transform, target_transform=None, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gn9k5T2fvLrz"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TrainCIFARDataset(Dataset):\n",
    "    \n",
    "\n",
    "    def __init__(self, file):\n",
    "        self.file = self.filter_dataset(file)\n",
    "    \n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.file)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.file[idx]\n",
    "\n",
    "    def replace_target_values(self ,t):\n",
    "        t = list(t)\n",
    "        targets = {8:0, 1:1}\n",
    "        t[1] = targets[t[1]]\n",
    "        return tuple(t)\n",
    "\n",
    "    def filter_dataset(self, data):\n",
    "        \"\"\"\n",
    "        filter the data to get only classes with ship and car, then replace the values of ships and car to 0,1\n",
    "        \"\"\"\n",
    "        targets = {8:0, 1:1}\n",
    "        filtered_ = [data[i] for i in range(len(data)) if data[i][1] in targets] \n",
    "        filtered_ = list(map(self.replace_target_values, filtered_)) \n",
    "        return filtered_\n",
    "\n",
    "class TestCIFARDataset(TrainCIFARDataset):\n",
    "    def __init__(self, file):\n",
    "        self.file = self.filter_dataset(file)\n",
    "    def __len__(self):\n",
    "        return len(self.file)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.file[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M8sGclaBq2Uy"
   },
   "outputs": [],
   "source": [
    "train_data = TrainCIFARDataset(train)\n",
    "train_loader = DataLoader(train_data, batch_size=1024)\n",
    "test_dataset = TestCIFARDataset(test)\n",
    "test_dataset = DataLoader(test_dataset, batch_size=1024)\n",
    "\n",
    "train_size = int(0.8 * len(train_data))\n",
    "test_size = len(train_data) - train_size\n",
    "\n",
    "\n",
    "train_dataset, val_dataset = random_split(train_data, [train_size, test_size])\n",
    "train_dataset, val_dataset = DataLoader(train_dataset, batch_size=64), DataLoader(val_dataset, batch_size= 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u9cYIJe5gqaa"
   },
   "source": [
    "** **bold text**Task 2. Logistic regression hypothesis** \n",
    "\n",
    "In order to map predicted values to probabilities, logistic regression needs a function which returns values between 0 and 1. Logistic function is used in this case. This function maps any real value into another value between 0 and 1. In machine learning, it is also referred to as sigmoid and is used to map predictions to probabilities.\n",
    "\n",
    "$f(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "![Image of logistic regression function](https://en.wikipedia.org/wiki/Logistic_function#/media/File:Logistic-curve.svg)\n",
    "\n",
    "the following function returns the sigmoid of a given input using torch utilities. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zDjlVHR0iPNM"
   },
   "outputs": [],
   "source": [
    "# TODO find and return the sigmoid of x\n",
    "def sigmoid(x):\n",
    "    m = nn.Sigmoid()\n",
    "    return m(x)\n",
    "    #another implementatios is return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yUCQxKoBo8i9"
   },
   "source": [
    "** **bold text**Task 3. Loss function** \n",
    "\n",
    "\n",
    "A common loss function used when dealing with probabilities in binary classification is binary cross entropy loss.\n",
    "\n",
    "$cross\\_entropy\\_loss(y, \\hat y) = \\frac{1}{N} \\sum_{i=0}^{N} y\\log \\hat y_{i} + (1-y)\\log (1 - \\hat y_{i})$\n",
    "\n",
    "For binary cross entropy loss, the number of classes is 2.\n",
    "\n",
    "Read about cross entropy in this [link](https://en.wikipedia.org/wiki/Cross_entropy).\n",
    "\n",
    "we do not have to worry also about implementing that ourselves. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A225CJsrpN78"
   },
   "outputs": [],
   "source": [
    "# TODO : compute mean binary cross entropy loss given a list of predicted and true labels\n",
    "def bce(y_true, y_pred):\n",
    "    loss = nn.BCELoss()\n",
    "    bce_loss = loss(y_pred, y_true) \n",
    "    #another implementation: bce_loss = torch.mean(y_true * torch.log(y_pred) + (1-y_true)*torch.log(1-y_pred))\n",
    "\n",
    "    return bce_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_TpZJqwYp-V6"
   },
   "source": [
    "**Task 4. Gradient descent to minimize the loss** \n",
    "\n",
    "The logistic regression parameter need to be optimized to minimize the loss function.\n",
    "\n",
    "We have the output of the logistic regression given a vector **x** as follows.\n",
    "\n",
    "$f(x) = \\frac{1}{1 + e^{-wx}}$\n",
    "\n",
    "the following function calculates the gradient of binary cross entropy loss function with respect to the parameter w. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WzF5Omjkqw6w"
   },
   "outputs": [],
   "source": [
    "def gradient(loss):\n",
    "  # TODO : compute and return the gradient of loss w.r.t the weight parameter\n",
    "  \n",
    "  return loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "66ynkN5TuPsQ"
   },
   "source": [
    "**Task 5 . Fitting the model** [5 point]\n",
    "\n",
    "the function below  fits a logistic regression model on the given input data with the specified learning rate and number of epochs using stochastic gradient descent.\n",
    "\n",
    "Follow the steps below to complete the function.\n",
    "```\n",
    "For each epoch:\n",
    "  For each mini batch:\n",
    "    1. Compute the predicted probabilities for all samples in the batch (y_pred)\n",
    "    2. Compute the predicted probabilities for all samples in the batch (y_pred).\n",
    "    3. Compute mean loss of the batch using function defined in task 9.\n",
    "    4. Compute the gradient of the loss w.r.t the weight parameter. use functin defined in task 10.\n",
    "    5. Keep track of the mean loss during each epoch.\n",
    "    6. Update weight parameter using stochastic gradient descent. The batch size is 64, according to the dataloaders defined in task 6. \n",
    "Return the loss and the optimized weight parameter.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wQ1VcC0vuXuq"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def fit(data, epochs, learning_rate):\n",
    "        \n",
    "    # TODO: get the data points and corresponding labels\n",
    "    #   x, y = [sample for sample in data]\n",
    "    '''TODO: The input x, which is multidimensional in this case, is multiplied\n",
    "     with the logistic regression parameter W to get a scalar. This is then \n",
    "     passed to the sigmoid function to get the probability. Use a tensor of \n",
    "     the required shape to initialize the weight parameter\n",
    "    '''\n",
    "    num_of_batches = len(data)\n",
    "    weight = torch.randn(1,3072, requires_grad= True)\n",
    "    # looping over the data   \n",
    "    for epoch in range(epochs):  \n",
    "        net_loss = 0.0 \n",
    "        # for each mini batch\n",
    "        for batch_x, batch_y in data:\n",
    "            x_ = batch_x.view(64,3072) # 3 x 32 x 32\n",
    "            z = torch.mm(weight, x_.t())\n",
    "            y_pred = sigmoid(z)\n",
    "            y_true = batch_y.view(1, 64).float()\n",
    "            loss = bce(y_true,y_pred)\n",
    "            net_loss += loss\n",
    "\n",
    "            # TODO: compute the gradient of the loss w.r.t weight\n",
    "            gradient(loss)\n",
    "            # TODO : perform one step of stochastic gradient descent to update weight\n",
    "            with torch.no_grad():\n",
    "                weight -= learning_rate * weight.grad\n",
    "                weight.grad.zero_()\n",
    "            \n",
    "\n",
    "\n",
    "        print('epoch: %d net_loss: %6.3f'%(epoch, net_loss/num_of_batches))\n",
    "        \n",
    "    return weight, net_loss/num_of_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CXP6HkbfsWaq"
   },
   "source": [
    "**Task 6. Hyperparameter tuning** \n",
    "\n",
    "The learning rate and the number of epochs are important hyperparameters that need to be set before training. \n",
    "Complete the function below to select the best hyperparameters given the list of possible combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4r_1wj_Bs2av"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "we implemented predict() function before its task becuase we will \n",
    "use it in predicting the data in validation dataset in order to fine-tune the hyperparams \n",
    "\"\"\"\n",
    "def predict(x, weight):\n",
    "    z = torch.mm(weight, x.t())\n",
    "    z = sigmoid(z)\n",
    "    return z\n",
    "\n",
    "def get_validation_accuracy(weight):\n",
    "    acc = 0.0\n",
    "    n = 0\n",
    "    for image, label in val_dataset:\n",
    "        n += label.shape[0] \n",
    "        out = predict(image, weight)\n",
    "        out = (out>0.5)\n",
    "        correct = (out == label).float().sum()\n",
    "        acc+= correct\n",
    "    acc = acc/n\n",
    "    return acc\n",
    "\n",
    "\n",
    "def select_best_hyperparams(data, learning_rates, epochs):\n",
    "  # TODO : initialize best loss\n",
    "    best_val_acc = 0\n",
    "    best_hyperparams= None\n",
    "\n",
    "    for learning_rate, epoch in zip(learning_rates, epochs):\n",
    "        np.random.seed(0)\n",
    "        # TODO find the hyperparameter combination which returns the minimum loss after training (using fit function)\n",
    "        weight, loss = fit(data,epoch, learning_rate)\n",
    "        validation_acc = get_validation_accuracy(weight)\n",
    "        if validation_acc > best_val_acc:\n",
    "            print(\"changing best val acc from: {0} to {1}\".format(best_val_acc, validation_acc))\n",
    "            best_val_acc = validation_acc\n",
    "            best_hyperparams = [epoch, learning_rate]\n",
    "                 \n",
    "    return best_hyperparams, best_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UxNvAwvwy_ks"
   },
   "source": [
    "**Task 7. Training using the best hyperparameters** [0.5 point]\n",
    "\n",
    "Complete the code below to select the best hyperparamater combination and then fit the training data using the selected learning rate and number of epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9GR2U4pEzP17",
    "outputId": "5ebd5a48-4fe9-4148-d790-8515ff7add1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 net_loss: 13.891\n",
      "epoch: 1 net_loss: 12.061\n",
      "epoch: 2 net_loss: 10.455\n",
      "epoch: 3 net_loss:  9.307\n",
      "epoch: 4 net_loss:  8.513\n",
      "epoch: 5 net_loss:  7.938\n",
      "epoch: 6 net_loss:  7.515\n",
      "epoch: 7 net_loss:  7.228\n",
      "epoch: 8 net_loss:  6.942\n",
      "epoch: 9 net_loss:  6.712\n",
      "epoch: 10 net_loss:  6.520\n",
      "epoch: 11 net_loss:  6.374\n",
      "epoch: 12 net_loss:  6.267\n",
      "epoch: 13 net_loss:  6.179\n",
      "epoch: 14 net_loss:  6.046\n",
      "epoch: 15 net_loss:  5.965\n",
      "epoch: 16 net_loss:  5.880\n",
      "epoch: 17 net_loss:  5.787\n",
      "epoch: 18 net_loss:  5.696\n",
      "epoch: 19 net_loss:  5.619\n",
      "epoch: 20 net_loss:  5.544\n",
      "epoch: 21 net_loss:  5.474\n",
      "epoch: 22 net_loss:  5.411\n",
      "epoch: 23 net_loss:  5.329\n",
      "epoch: 24 net_loss:  5.241\n",
      "epoch: 0 net_loss:  9.941\n",
      "epoch: 1 net_loss:  6.750\n",
      "epoch: 2 net_loss:  6.196\n",
      "epoch: 3 net_loss:  5.875\n",
      "epoch: 4 net_loss:  5.393\n",
      "epoch: 5 net_loss:  5.137\n",
      "epoch: 6 net_loss:  4.828\n",
      "epoch: 7 net_loss:  4.480\n",
      "epoch: 8 net_loss:  4.258\n",
      "epoch: 9 net_loss:  4.032\n",
      "epoch: 10 net_loss:  3.892\n",
      "epoch: 11 net_loss:  3.696\n",
      "epoch: 12 net_loss:  3.524\n",
      "epoch: 13 net_loss:  3.377\n",
      "epoch: 14 net_loss:  3.294\n",
      "epoch: 15 net_loss:  3.199\n",
      "epoch: 16 net_loss:  3.088\n",
      "epoch: 17 net_loss:  2.995\n",
      "epoch: 18 net_loss:  2.925\n",
      "epoch: 19 net_loss:  2.841\n",
      "epoch: 20 net_loss:  2.821\n",
      "epoch: 21 net_loss:  2.674\n",
      "epoch: 22 net_loss:  2.661\n",
      "epoch: 23 net_loss:  2.618\n",
      "epoch: 24 net_loss:  2.559\n",
      "epoch: 25 net_loss:  2.473\n",
      "epoch: 26 net_loss:  2.397\n",
      "epoch: 27 net_loss:  2.447\n",
      "epoch: 28 net_loss:  2.434\n",
      "epoch: 29 net_loss:  2.386\n",
      "epoch: 30 net_loss:  2.354\n",
      "epoch: 31 net_loss:  2.365\n",
      "epoch: 32 net_loss:  2.365\n",
      "epoch: 33 net_loss:  2.377\n",
      "epoch: 34 net_loss:  2.301\n",
      "epoch: 35 net_loss:  2.310\n",
      "epoch: 36 net_loss:  2.239\n",
      "epoch: 37 net_loss:  2.243\n",
      "epoch: 38 net_loss:  2.190\n",
      "epoch: 39 net_loss:  2.132\n",
      "epoch: 40 net_loss:  2.208\n",
      "epoch: 41 net_loss:  2.192\n",
      "epoch: 42 net_loss:  2.122\n",
      "epoch: 43 net_loss:  2.177\n",
      "epoch: 44 net_loss:  2.098\n",
      "epoch: 45 net_loss:  2.092\n",
      "epoch: 46 net_loss:  2.026\n",
      "epoch: 47 net_loss:  2.032\n",
      "epoch: 48 net_loss:  2.057\n",
      "epoch: 49 net_loss:  1.983\n",
      "epoch: 0 net_loss:  8.491\n",
      "epoch: 1 net_loss:  7.000\n",
      "epoch: 2 net_loss:  6.947\n",
      "epoch: 3 net_loss:  6.529\n",
      "epoch: 4 net_loss:  6.373\n",
      "epoch: 5 net_loss:  6.140\n",
      "epoch: 6 net_loss:  5.822\n",
      "epoch: 7 net_loss:  5.949\n",
      "epoch: 8 net_loss:  5.776\n",
      "epoch: 9 net_loss:  5.809\n",
      "epoch: 10 net_loss:  5.871\n",
      "epoch: 11 net_loss:  5.848\n",
      "epoch: 12 net_loss:  5.587\n",
      "epoch: 13 net_loss:  5.608\n",
      "epoch: 14 net_loss:  5.736\n",
      "epoch: 15 net_loss:  5.414\n",
      "epoch: 16 net_loss:  5.331\n",
      "epoch: 17 net_loss:  5.290\n",
      "epoch: 18 net_loss:  5.377\n",
      "epoch: 19 net_loss:  5.181\n",
      "epoch: 20 net_loss:  5.355\n",
      "epoch: 21 net_loss:  5.260\n",
      "epoch: 22 net_loss:  5.138\n",
      "epoch: 23 net_loss:  5.319\n",
      "epoch: 24 net_loss:  5.141\n",
      "epoch: 25 net_loss:  5.022\n",
      "epoch: 26 net_loss:  5.161\n",
      "epoch: 27 net_loss:  5.255\n",
      "epoch: 28 net_loss:  5.156\n",
      "epoch: 29 net_loss:  5.159\n",
      "epoch: 30 net_loss:  4.988\n",
      "epoch: 31 net_loss:  4.765\n",
      "epoch: 32 net_loss:  5.110\n",
      "epoch: 33 net_loss:  4.892\n",
      "epoch: 34 net_loss:  4.739\n",
      "epoch: 35 net_loss:  4.870\n",
      "epoch: 36 net_loss:  4.739\n",
      "epoch: 37 net_loss:  4.979\n",
      "epoch: 38 net_loss:  4.830\n",
      "epoch: 39 net_loss:  4.836\n",
      "epoch: 40 net_loss:  4.702\n",
      "epoch: 41 net_loss:  4.504\n",
      "epoch: 42 net_loss:  4.687\n",
      "epoch: 43 net_loss:  4.844\n",
      "epoch: 44 net_loss:  4.548\n",
      "epoch: 45 net_loss:  4.767\n",
      "epoch: 46 net_loss:  4.755\n",
      "epoch: 47 net_loss:  4.724\n",
      "epoch: 48 net_loss:  4.555\n",
      "epoch: 49 net_loss:  4.651\n",
      "epoch: 50 net_loss:  4.524\n",
      "epoch: 51 net_loss:  4.358\n",
      "epoch: 52 net_loss:  4.425\n",
      "epoch: 53 net_loss:  4.421\n",
      "epoch: 54 net_loss:  4.469\n",
      "epoch: 55 net_loss:  4.246\n",
      "epoch: 56 net_loss:  4.225\n",
      "epoch: 57 net_loss:  4.371\n",
      "epoch: 58 net_loss:  4.336\n",
      "epoch: 59 net_loss:  4.439\n",
      "epoch: 60 net_loss:  4.316\n",
      "epoch: 61 net_loss:  4.525\n",
      "epoch: 62 net_loss:  4.378\n",
      "epoch: 63 net_loss:  4.289\n",
      "epoch: 64 net_loss:  4.547\n",
      "epoch: 65 net_loss:  4.415\n",
      "epoch: 66 net_loss:  4.230\n",
      "epoch: 67 net_loss:  4.429\n",
      "epoch: 68 net_loss:  4.246\n",
      "epoch: 69 net_loss:  4.481\n",
      "epoch: 70 net_loss:  4.408\n",
      "epoch: 71 net_loss:  4.304\n",
      "epoch: 72 net_loss:  4.330\n",
      "epoch: 73 net_loss:  4.218\n",
      "epoch: 74 net_loss:  4.138\n",
      "epoch: 75 net_loss:  4.164\n",
      "epoch: 76 net_loss:  4.318\n",
      "epoch: 77 net_loss:  4.232\n",
      "epoch: 78 net_loss:  4.315\n",
      "epoch: 79 net_loss:  4.192\n",
      "epoch: 80 net_loss:  4.221\n",
      "epoch: 81 net_loss:  4.171\n",
      "epoch: 82 net_loss:  4.238\n",
      "epoch: 83 net_loss:  4.265\n",
      "epoch: 84 net_loss:  4.058\n",
      "epoch: 85 net_loss:  4.209\n",
      "epoch: 86 net_loss:  4.118\n",
      "epoch: 87 net_loss:  4.009\n",
      "epoch: 88 net_loss:  4.144\n",
      "epoch: 89 net_loss:  4.028\n",
      "epoch: 90 net_loss:  4.174\n",
      "epoch: 91 net_loss:  4.250\n",
      "epoch: 92 net_loss:  4.254\n",
      "epoch: 93 net_loss:  4.060\n",
      "epoch: 94 net_loss:  3.979\n",
      "epoch: 95 net_loss:  4.071\n",
      "epoch: 96 net_loss:  4.136\n",
      "epoch: 97 net_loss:  3.997\n",
      "epoch: 98 net_loss:  4.150\n",
      "epoch: 99 net_loss:  3.950\n",
      "Best hyperparameters using validation data.\n",
      "Learning rate: 0.100, Number of epochs: 50 \n"
     ]
    }
   ],
   "source": [
    "# hyperparameters combinations \n",
    "\n",
    "np.random.seed(0) #to reproduce the results \n",
    "learning_rates = [0.01, 0.1, 1] \n",
    "epochs = [25, 50, 100]\n",
    "# TODO : use the function defined in task 12 to find the best hyperparameter combination from the above list\n",
    "best_hyperparams , best_loss= select_best_hyperparams(train_dataset,learning_rates, epochs)\n",
    "\n",
    "print('Best hyperparameters using validation data.\\nLearning rate: %5.3f, Number of epochs: %d '% (best_hyperparams[1], best_hyperparams[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xBAd61cVhce8"
   },
   "source": [
    "**Task 8. Logistic regression threshold** \n",
    "\n",
    "\n",
    "Logistic regression takes an input and returns a values between 0 and 1. To interpret this output as a probability of the input being in a class, we need to define a threshold. We set a threshold of 0.5.\n",
    "\n",
    "We predict class 0 if f(x) is greater than or equal to 0.5, else we predict the data point to be of an instance of class 1.\n",
    "\n",
    "the following function[link text](https://) predicts the class (ship or car) of a given input. [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 885
    },
    "colab_type": "code",
    "id": "GgfTLnN9hrZR",
    "outputId": "28509f5a-2cc4-4170-c34b-79113269b25e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 net_loss:  9.312\n",
      "epoch: 1 net_loss:  6.980\n",
      "epoch: 2 net_loss:  6.266\n",
      "epoch: 3 net_loss:  5.766\n",
      "epoch: 4 net_loss:  5.301\n",
      "epoch: 5 net_loss:  4.920\n",
      "epoch: 6 net_loss:  4.584\n",
      "epoch: 7 net_loss:  4.222\n",
      "epoch: 8 net_loss:  3.962\n",
      "epoch: 9 net_loss:  3.722\n",
      "epoch: 10 net_loss:  3.538\n",
      "epoch: 11 net_loss:  3.402\n",
      "epoch: 12 net_loss:  3.342\n",
      "epoch: 13 net_loss:  3.225\n",
      "epoch: 14 net_loss:  3.164\n",
      "epoch: 15 net_loss:  3.101\n",
      "epoch: 16 net_loss:  3.075\n",
      "epoch: 17 net_loss:  2.973\n",
      "epoch: 18 net_loss:  2.890\n",
      "epoch: 19 net_loss:  2.840\n",
      "epoch: 20 net_loss:  2.688\n",
      "epoch: 21 net_loss:  2.758\n",
      "epoch: 22 net_loss:  2.722\n",
      "epoch: 23 net_loss:  2.646\n",
      "epoch: 24 net_loss:  2.590\n",
      "epoch: 25 net_loss:  2.545\n",
      "epoch: 26 net_loss:  2.502\n",
      "epoch: 27 net_loss:  2.455\n",
      "epoch: 28 net_loss:  2.435\n",
      "epoch: 29 net_loss:  2.421\n",
      "epoch: 30 net_loss:  2.340\n",
      "epoch: 31 net_loss:  2.344\n",
      "epoch: 32 net_loss:  2.288\n",
      "epoch: 33 net_loss:  2.274\n",
      "epoch: 34 net_loss:  2.290\n",
      "epoch: 35 net_loss:  2.260\n",
      "epoch: 36 net_loss:  2.248\n",
      "epoch: 37 net_loss:  2.180\n",
      "epoch: 38 net_loss:  2.170\n",
      "epoch: 39 net_loss:  2.148\n",
      "epoch: 40 net_loss:  2.160\n",
      "epoch: 41 net_loss:  2.158\n",
      "epoch: 42 net_loss:  2.134\n",
      "epoch: 43 net_loss:  2.105\n",
      "epoch: 44 net_loss:  2.047\n",
      "epoch: 45 net_loss:  2.083\n",
      "epoch: 46 net_loss:  2.019\n",
      "epoch: 47 net_loss:  2.112\n",
      "epoch: 48 net_loss:  2.077\n",
      "epoch: 49 net_loss:  2.063\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "l_Rate, epoch = best_hyperparams[1], best_hyperparams[0]\n",
    "weight, loss = fit(train_dataset,epoch, l_Rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bFD9TuHPGSc3"
   },
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "  # TODO : compute the predicted class label\n",
    "    z = torch.mm(weight, x.t())\n",
    "    z = sigmoid(z)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UDVYSavWzRJu"
   },
   "source": [
    "**Task 9. Reporting accuracy on test set** [*0.5* point]\n",
    "\n",
    "The test set is used to give an indication of the generalization abilities of the model, that is to estimate how good the model is over random guessing at an unseen data point.\n",
    "\n",
    "the code below computes the accuracy of logistic regression model on the test set. For this, first bring the test set to the low dimensional subspace and then make predictions using the trained model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "rFhixtHzhTmM",
    "outputId": "3e403d0c-79c7-4f81-b423-9e58792718ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "Accuracy on the test set :  0.754\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: compute the accuracy on the reduced test set\n",
    "acc = 0.0\n",
    "n = 0\n",
    "for image, label in test_dataset:\n",
    "    n += label.shape[0] \n",
    "    image = image.view(image.size()[0],3*32*32)\n",
    "    out = predict(image)\n",
    "\n",
    "    out = (out>0.5)\n",
    "    correct = (out == label).float().sum()\n",
    "    acc+= correct\n",
    "print(n)\n",
    "acc = acc/n\n",
    "\n",
    "\n",
    "\n",
    "print('Accuracy on the test set : %6.3f'% (acc))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Abdallah_NNTI_Project1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
