{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Abdallah_NNTI_Project1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdallah197/neuralnetworks-project/blob/master/Abdallah_NNTI_Project1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeWYgXklaPdY",
        "colab_type": "text"
      },
      "source": [
        "## **NNTI 19/20 Project 1:  PCA and Logistic Regression**\n",
        "\n",
        "## **Deadline: 17 December 2019, 23:59**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Important:** For all computations in this project, please use the torch library. The torch package contains data structures for multi-dimensional tensors and mathematical operations over these are defined. Additionally, it provides many utilities for efficient serializing of Tensors and arbitrary types, and other useful utilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTdw-cKkajiG",
        "colab_type": "text"
      },
      "source": [
        "# **1. Principal component analysis (PCA) [12 points]**\n",
        "\n",
        "Principal component analysis (PCA) is a technique used to emphasize variation and bring out strong patterns in a dataset. It's often used to make data easy to explore and visualize. PCA is also a very useful dimesnionality reduction technique. In the folowing, we explore how to apply PCA on the CIFAR dataset.\n",
        "\n",
        "CIFAR 10 is a collection images which is commonly used to train machine learning and computer vision algorithms. This dataset contains 50000 training images and 10000 validation images such that the images can be classified between 10 different classes.  The images in CIFAR-10 are of size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.\n",
        "\n",
        "**Task 1. Getting the dataset using torchvision**\n",
        "\n",
        "Torchvision is a pytorch package which helps in loading datasets in the image domain. It has dataloaders for common datasets like CIFAR 10, MNIST etc. Complete the code below to download the CIFAR dataset using torchvision. Torchvision returns the dataset in which every image is stretched out into a 3072-dimensional row vector.\n",
        "\n",
        "Complete the code below to load the CIFAR dataset using torchvision. Print the labels and some images with the corresponding labels.\n",
        "[1.5 points]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwqJY7-3aHhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# TODO: define transformation function using torchvision.transforms to convert images into pytorch tensors\n",
        "transform = None\n",
        "\n",
        "# TODO: use torchvision.datasets to load CIFAR-10 train dataset and the defined transform\n",
        "train_dataset = None\n",
        "\n",
        "# TODO: use torch.utils.data.DataLoader to get a python Iterable over the dataset, use batch_size = 20\n",
        "train_loader = None\n",
        "\n",
        "# TODO: get the first batch of images of the dataset using python Iterator\n",
        "train_iter = None\n",
        "images, labels = None, None\n",
        "\n",
        "# TODO: plot the first batch of images of the training dataset\n",
        "\n",
        "# the labels are numbered from 0 - 9 as follows\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9f6Y4Bn0w2p",
        "colab_type": "text"
      },
      "source": [
        "**Points**: $0.0$ of $1.5$\n",
        "**Comments**: None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyvRl1ZJdm8_",
        "colab_type": "text"
      },
      "source": [
        "**Task 2. Centering and normalization** \n",
        "\n",
        "PCA is applied on data after centering and normalization.\n",
        "Complete the functions below to center and normalize the images. \n",
        "\n",
        "[1 point]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1rB6TL7LLUiJ",
        "colab": {}
      },
      "source": [
        "def center(images):\n",
        "    # TODO : substract the mean from the images\n",
        "    mean = torch.mean(images)\n",
        "    centered_images = images - mean\n",
        "    return centered_images\n",
        "\n",
        "def normalize(images):\n",
        "    # TODO : normalize the centered images by dividing by the standard deviation\n",
        "    std = torch.std(images)\n",
        "    normalized_images = images/std\n",
        "    return normalized_images\n",
        "\n",
        "def transform(images):\n",
        "  # reshaping the images as a 3072-dimensional row vector\n",
        "  images = images.reshape((images.shape[0], 3072))\n",
        "  images = center(images)\n",
        "  images = normalize(images)\n",
        "  return images\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFsqITKA0uWO",
        "colab_type": "text"
      },
      "source": [
        "**Points**: $0.0$ of $1$\n",
        "**Comments**: None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__al7OB3hSip",
        "colab_type": "text"
      },
      "source": [
        "**Task 3. Implementing PCA** \n",
        "\n",
        "PCA takes in the data points and the target dimension which is lesser than the original dimension of the data. In this case, the data matrix is of [number of images, 3072]. \n",
        "\n",
        "The following presents the main steps to perform a slightly modified version of PCA. This function takes as input the original data points, and one of the two parameters: target_dim or target_variance. If target_dim is given then return the projected data into a target_dim-dimensional space. Otherwise, the function returns data projected into a low dimensional space which captures a ratio of target_variance from the data. \n",
        "\n",
        "\n",
        "1.   Trasnform the data by centering and normalizing the images\n",
        "2.   Get the data matrix of shape [#images, #dimensions] \n",
        "3.   Compute the covariance matrix of the data matrix\n",
        "4.   Compute the eigenvectors and eigenvalues of the covariance matrix\n",
        "5.   Sort eigenvectors by decreasing eigenvalues\n",
        "6.   If target_variance is given then compute the target_dim corresponding to it.\n",
        "7.   Select the top target_dim eigenvectors to get the encoding matrix of shape [target_dim, #dimesnions]\n",
        "6.   Multiply the data matrix with the encoding matrix to project the data into the low dimensional space\n",
        "\n",
        "Complete the function below to implement PCA and return the reduced dimension set and captured variance.\n",
        "\n",
        "[8 points]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHJ782tviRS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "'''\n",
        "PCA function must take the original images and either the number of target\n",
        "dimensions or the ratio of the total variance to be captured from the data\n",
        "\n",
        "'''\n",
        "def PCA(images, target_dim = 0, target_variance = 0): \n",
        "  start = time.time()\n",
        "  # transform the data\n",
        "  data = transform(images)\n",
        "  # TODO : compute the covariance matrix of the data points\n",
        "  # cov = torch.from_numpy(np.cov(data.t()))\n",
        "  cov = torch.mm(data.T, data)/data.size()[0]\n",
        "  # print(\"cov at: {0}\".format(time.time() - start))\n",
        "  # TODO: compute the eigenvectors and eigenvalues of the covariance matrix\n",
        "  w, lamb = torch.eig(cov, True)\n",
        "  eigvecs = lamb\n",
        "  eigvals = w[:,0]\n",
        "  # print(\"eig at: {0}\".format(time.time() - start))\n",
        "  # TODO: sort eigenvectors by decreasing eigenvalues\n",
        "  sorted_eigvals_idx = torch.argsort(eigvals)\n",
        "  sorted_eigvecs = torch.index_select(eigvecs, 1, sorted_eigvals_idx)\n",
        "  # print(\"sorted at: {0}\".format(time.time() - start))\n",
        "  \n",
        "  # sorted.\n",
        "  if(target_variance != 0):\n",
        "    # TODO: compute target_dim such that target_variance is captured from the data\n",
        "    sorted_eigvals, _ = torch.sort(eigvals)\n",
        "    total_var = torch.sum(sorted_eigvals)\n",
        "\n",
        "    current_var = 0\n",
        "    target_dim = 0\n",
        "    i = len(sorted_eigvals) - 1\n",
        "\n",
        "    while current_var <= (target_variance * total_var) :\n",
        "      target_dim += 1\n",
        "      current_var += sorted_eigvals[i]\n",
        "      i -= 1\n",
        "    print(\"target variance at: {0}\".format(time.time() - start))\n",
        "    print(\"Changed target_dim to: {0}\".format(target_dim))\n",
        "  \n",
        "  # TODO: choose $target_dim eigenvectors corresponding to the low dimensional subspace\n",
        "  to_cat = []\n",
        "  for i in range(target_dim):\n",
        "    temp = sorted_eigvecs[-i].reshape(3072, 1)\n",
        "    to_cat.append(temp)\n",
        "  encoding_matrix = torch.cat(to_cat, dim=1)\n",
        "\n",
        "  # TODO: multiply the data matrix with the encoding matrix to get the reduced dataset\n",
        "  reduced_data = data.mm(encoding_matrix.float())\n",
        "\n",
        "  return reduced_data, target_dim, encoding_matrix\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq5IVNLP0sHu",
        "colab_type": "text"
      },
      "source": [
        "**Points**: $0.0$ of $8$\n",
        "**Comments**: None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wrSjkq4nmnG",
        "colab_type": "text"
      },
      "source": [
        "**Task 4. PCA for dimensionality reduction** \n",
        "\n",
        "PCA is normally used to bring to acheive dimensionality reduction for high       dimensional datasets. This is acheived by bringing the dataset into a low dimensional subspace while still capturing most of the variance in the dataset. Use the above function to reduce the dataset into a 50 dimensional subspace.  \n",
        "\n",
        "[0.5 point]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDm9URGwwnvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO : use dataloader with batch size 2000 to load the dataset\n",
        "train_loader = None\n",
        "# TODO : apply PCA with target_d = 50 on the first batch (contains 2000 images) from the dataloader\n",
        "reduced_data = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OVvAJm20pof",
        "colab_type": "text"
      },
      "source": [
        "**Points**: $0.0$ of $0.5$\n",
        "**Comments**: None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHQhqJ-yNKyR",
        "colab_type": "text"
      },
      "source": [
        "**Task 5. PCA for visualization**\n",
        "\n",
        "PCA is often used for visualization purposes.  Visually exploring the data can become challenging when we have more than 3 features. But this can be a very useful tool when dealing with data related problems. \n",
        "\n",
        "Please followe the steps below and complete the folowing code to create a scatterplot of the first and second principal components. Make use of matplotlib for the following.\n",
        "\n",
        "1.   Create a 2D scatter plot. For each data point, plot the first principle component on $x$ axis and the second principle component on $y$ axis, use different colors for each class.\n",
        "2.   Set corresponding labels: assign label \"first principle component\" for $x$ axis and \"second principle component\" for $y$ axis.\n",
        "3.   Add legends for each class.\n",
        " \n",
        "Are the first two components discriminative enough to classify points from any pair of the ten classes ?\n",
        "[1 point]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0e9VFy3Nl0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a scatterplot of the first two dimensions of the reduced data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-E8fLlDr0nhI",
        "colab_type": "text"
      },
      "source": [
        "**Points**: $0.0$ of $1$\n",
        "**Comments**: None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEMLHKckbrpw",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# **2. Logistic regression [18 points]**\n",
        "\n",
        "\n",
        "Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes.\n",
        "\n",
        "In this section, we explore the binary logistic regression. We change the clsssification problem with 10 classes into a binary classification problem by considering only the points from the classes *'ship'* and *'car'*.\n",
        "\n",
        "![Logistic regression image](https://drive.google.com/uc?id=1eRF1-2qnQYAkkCDpAwROj5MiMKzct2x0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsvjieqtuRnx",
        "colab_type": "text"
      },
      "source": [
        "**Task 6. Generating training dataset** \n",
        "\n",
        "Since logistic regression is a classification problem with two classes, we need a labelled dataset with two classes as the training set. Complete the function to obtain the datapoints corresponding to labels *'ship'* and *'car'*. \n",
        "\n",
        "Please follow the steps below: \n",
        "\n",
        "1. Load the train and test sets of CIFAR 10 from torchvision using a batch size of 1024.\n",
        "2. Split the training samples by 80:20 ratio into train set and validation set respectively. \n",
        "3. Filter the datasets to only have images with classes 'ship' or 'car'. The corresponding labels are 8 and 1 respectively.\n",
        "4. The new labels for binary classification problem should be 'ship' : 0 and 'car': 1.\n",
        "5. Define a [torch.utils.data.Dataset](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html) with the filtered tensors of images and the newly created labels.\n",
        "6. Define a dataloader for training and validation datasets with batch_size 64.\n",
        "\n",
        "[1.5 point]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlZ6WVYBugBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO : generate the train, validation, and test sets from CIFAR 10  \n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms, utils, datasets\n",
        "from torch import nn,autograd\n",
        "from torch.autograd import Variable\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v54PSq9Eq87-",
        "colab_type": "code",
        "outputId": "8abbb5af-c64d-4332-a3fd-9f1fbdb251de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #mean, std for Red, Green and Blue\n",
        "])\n",
        "train = datasets.CIFAR10(root='data/cifar/', train=True, transform=transform, target_transform=None, download=True)\n",
        "test =datasets.CIFAR10(root='data/cifar/', train=False, transform=transform, target_transform=None, download=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:06, 27175875.93it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/cifar/cifar-10-python.tar.gz to data/cifar/\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn9k5T2fvLrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class TrainCIFARDataset(Dataset):\n",
        "    \n",
        "\n",
        "    def __init__(self, file):\n",
        "        self.file = self.filter_dataset(file)\n",
        "    \n",
        " \n",
        "    def __len__(self):\n",
        "        return len(self.file)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.file[idx]\n",
        "\n",
        "    def replace_target_values(self ,t):\n",
        "        t = list(t)\n",
        "        targets = {8:0, 1:1}\n",
        "        t[1] = targets[t[1]]\n",
        "        return tuple(t)\n",
        "\n",
        "    def filter_dataset(self, data):\n",
        "        \"\"\"\n",
        "        filter the data to get only classes with ship and car, then replace the values of ships and car to 0,1\n",
        "        \"\"\"\n",
        "        targets = {8:0, 1:1}\n",
        "        filtered_ = [data[i] for i in range(len(data)) if data[i][1] in targets] \n",
        "        filtered_ = list(map(self.replace_target_values, filtered_)) \n",
        "        return filtered_\n",
        "\n",
        "class TestCIFARDataset(TrainCIFARDataset):\n",
        "    def __init__(self, file):\n",
        "        self.file = self.filter_dataset(file)\n",
        "    def __len__(self):\n",
        "        return len(self.file)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.file[idx]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8sGclaBq2Uy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = TrainCIFARDataset(train)\n",
        "train_loader = DataLoader(train_data, batch_size=1024)\n",
        "test_dataset = TestCIFARDataset(test)\n",
        "test_dataset = DataLoader(test_dataset, batch_size=1024)\n",
        "\n",
        "train_size = int(0.8 * len(train_data))\n",
        "test_size = len(train_data) - train_size\n",
        "\n",
        "\n",
        "train_dataset, val_dataset = random_split(train_data, [train_size, test_size])\n",
        "train_dataset, val_dataset = DataLoader(train_dataset, batch_size=64), DataLoader(val_dataset, batch_size= 64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOFrsgn33PAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for batch, label in train_dataset:\n",
        "#     print(batch,label)\n",
        "#     break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eZAUQGL0YE1",
        "colab_type": "text"
      },
      "source": [
        "**Points**: $0.0$ of $1.5$\n",
        "**Comments**: None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM5rUyrOuyco",
        "colab_type": "text"
      },
      "source": [
        "**Task 7. Get the dataset in the low dimensional subspace** \n",
        "\n",
        "Apply pca on the original data points to get new data matrix.\n",
        "The target dimensions must capture 90% of the variance in the data.\n",
        "Use the previously defined PCA function for this task. In the coming sections, we will be using this projected training dataset to training the logistic regression model.\n",
        "\n",
        "[0.5 point]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pZNW9Xpu81a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "e8a2713b-97b0-418c-aa35-2996df64868f"
      },
      "source": [
        "# TODO : use the pca function defined in task 3 to reduce dimensions of the train set \n",
        "reduced_data = PCA(images, target_variance=0.9)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-401f3676698a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreduced_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'PCA' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WABj2zBm0Wp3",
        "colab_type": "text"
      },
      "source": [
        "**Points**: $0.0$ of $0.5$\n",
        "**Comments**: None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u9cYIJe5gqaa"
      },
      "source": [
        "**Task 8. Logistic regression hypothesis** \n",
        "\n",
        "In order to map predicted values to probabilities, logistic regression needs a function which returns values between 0 and 1. Logistic function is used in this case. This function maps any real value into another value between 0 and 1. In machine learning, it is also referred to as sigmoid and is used to map predictions to probabilities.\n",
        "\n",
        "$f(x) = \\frac{1}{1 + e^{-x}}$\n",
        "\n",
        "![Image of logistic regression function](https://en.wikipedia.org/wiki/Logistic_function#/media/File:Logistic-curve.svg)\n",
        "\n",
        "Complete the following function which returns the sigmoid of a given input. \n",
        "\n",
        "[0.5 point]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDjlVHR0iPNM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "    m = nn.Sigmoid()\n",
        "    return m(x)\n",
        "    #another implementatios is return 1/(1+torch.exp(-x))\n",
        "  # TODO find and return the sigmoid of x\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKdGMZsI0VVN",
        "colab_type": "text"
      },
      "source": [
        "**Points**: $0.0$ of $0.5$\n",
        "**Comments**: None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUCQxKoBo8i9",
        "colab_type": "text"
      },
      "source": [
        "**Task 9. Loss function** \n",
        "\n",
        "\n",
        "A common loss function used when dealing with probabilities in binary classification is binary cross entropy loss.\n",
        "\n",
        "$cross\\_entropy\\_loss(y, \\hat y) = \\frac{1}{N} \\sum_{i=0}^{N} y\\log \\hat y_{i} + (1-y)\\log (1 - \\hat y_{i})$\n",
        "\n",
        "For binary cross entropy loss, the number of classes is 2.\n",
        "\n",
        "Read about cross entropy in this [link](https://en.wikipedia.org/wiki/Cross_entropy).\n",
        "\n",
        "Complete the following function to return the binary cross entropy loss. \n",
        "\n",
        "[1 point]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A225CJsrpN78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bce(y_true, y_pred):\n",
        "    loss = nn.BCELoss()\n",
        "    bce_loss = loss(y_pred, y_true) \n",
        "    # bce_loss = torch.mean(y_true * torch.log(y_pred) + (1-y_true)*torch.log(1-y_pred))\n",
        "  # TODO : compute mean binary cross entropy loss given a list of predicted and true labels\n",
        "#   bce_loss = torch.mean((torch.dot(y_true.float(), torch.log(y_pred))\n",
        "#        + torch.dot((torch.ones(y_true.size())-y_true), torch.log(torch.ones(y_true.size())-y_pred))\n",
        "#   ))\n",
        "\n",
        "    return bce_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb4zFXedz3Dy",
        "colab_type": "text"
      },
      "source": [
        "**Points**: $0.0$ of $1$\n",
        "**Comments**: None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TpZJqwYp-V6",
        "colab_type": "text"
      },
      "source": [
        "**Task 10. Gradient descent to minimize the loss** \n",
        "\n",
        "The logistic regression parameter need to be optimized to minimize the loss function.\n",
        "\n",
        "We have the output of the logistic regression given a vector **x** as follows.\n",
        "\n",
        "$f(x) = \\frac{1}{1 + e^{-wx}}$\n",
        "\n",
        "Complete the following function to calculate the gradient of binary cross entropy loss function with respect to the parameter w. \n",
        "\n",
        "[1 point]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzF5Omjkqw6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient(loss):\n",
        "  # TODO : compute and return the gradient of loss w.r.t the weight parameter\n",
        "  \n",
        "  return loss.backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRayzlRdz1VR",
        "colab_type": "text"
      },
      "source": [
        "**Points**: $0.0$ of $1$\n",
        "**Comments**: None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66ynkN5TuPsQ",
        "colab_type": "text"
      },
      "source": [
        "**Task 11 . Fitting the model** [5 point]\n",
        "\n",
        "Complete the function below which fits a logistic regression model on the given input data with the specified learning rate and number of epochs using stochastic gradient descent.\n",
        "\n",
        "Follow the steps below to complete the function.\n",
        "```\n",
        "For each epoch:\n",
        "  For each mini batch:\n",
        "    1. Compute the predicted probabilities for all samples in the batch (y_pred)\n",
        "    2. Compute the predicted probabilities for all samples in the batch (y_pred).\n",
        "    3. Compute mean loss of the batch using function defined in task 9.\n",
        "    4. Compute the gradient of the loss w.r.t the weight parameter. use functin defined in task 10.\n",
        "    5. Keep track of the mean loss during each epoch.\n",
        "    6. Update weight parameter using stochastic gradient descent. The batch size is 64, according to the dataloaders defined in task 6. \n",
        "Return the loss and the optimized weight parameter.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ1VcC0vuXuq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "def fit(data, epochs, learning_rate):\n",
        "    \n",
        "\n",
        "    \n",
        "    # TODO: get the data points and corresponding labels\n",
        "    #   x, y = [sample for sample in data]\n",
        "    '''TODO: The input x, which is multidimensional in this case, is multiplied\n",
        "     with the logistic regression parameter W to get a scalar. This is then \n",
        "     passed to the sigmoid function to get the probability. Use a tensor of \n",
        "     the required shape to initialize the weight parameter\n",
        "    '''\n",
        "    num_of_batches = len(data)\n",
        "    np.random.seed(0)\n",
        "    weight = torch.randn(1,3072, requires_grad= True)\n",
        "    # looping over the data   \n",
        "    for epoch in range(epochs):  \n",
        "        net_loss = 0.0 \n",
        "        # for each mini batch\n",
        "        for batch_x, batch_y in data:\n",
        "            x_ = batch_x.view(64,3072) # 3 x 32 x 32\n",
        "            # z = torch.mm(x_, weight)\n",
        "            # x_, target_dim, encoding_matrix = PCA(x_,target_dim=25)\n",
        "            # print(x_.size())\n",
        "            z = torch.mm(weight, x_.t())\n",
        "            y_pred = sigmoid(z)\n",
        "            # TODO: compute the mean loss of the batch\n",
        "            # y_pred = y_pred.view(64)\n",
        "            # y_pred  = torch.tensor(y_pred, dtype=torch.long)\n",
        "            y_true = batch_y.view(1, 64).float()\n",
        "            loss = bce(y_true,y_pred)\n",
        "            # print(loss)\n",
        "            net_loss += loss\n",
        "\n",
        "            # TODO: compute the gradient of the loss w.r.t weight\n",
        "            gradient(loss)\n",
        "            # TODO : perform one step of stochastic gradient descent to update weight\n",
        "            with torch.no_grad():\n",
        "                weight -= learning_rate * weight.grad\n",
        "                weight.grad.zero_()\n",
        "            \n",
        "\n",
        "\n",
        "        print('epoch: %d net_loss: %6.3f'%(epoch, net_loss/num_of_batches))\n",
        "        \n",
        "    return weight, net_loss/num_of_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmigYfCCzyze",
        "colab_type": "text"
      },
      "source": [
        "**Points**: $0.0$ of $5$\n",
        "**Comments**: None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXP6HkbfsWaq",
        "colab_type": "text"
      },
      "source": [
        "**Task 12. Hyperparameter tuning** \n",
        "\n",
        "The learning rate and the number of epochs are important hyperparameters that need to be set before training. \n",
        "Complete the function below to select the best hyperparameters given the list of possible combinations.\n",
        "\n",
        "[1.5 point]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r_1wj_Bs2av",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def select_best_hyperparams(data, learning_rates, epochs):\n",
        "  # TODO : initialize best loss\n",
        "    best_loss = 0\n",
        "    hyperparams = {}\n",
        "\n",
        "    for learning_rate, epoch in zip(learning_rates, epochs):\n",
        "        np.random.seed(0)\n",
        "        # TODO find the hyperparameter combination which returns the minimum loss after training (using fit function)\n",
        "        weight, loss = fit(data,epoch, learning_rate)\n",
        "        hyperparams[loss]= [epoch,learning_rate]    \n",
        "         \n",
        "    best_hyperparams = hyperparams[min(hyperparams)]\n",
        "    best_loss = min(hyperparams)\n",
        "    return best_hyperparams, best_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N8DdD_ezuRk",
        "colab_type": "text"
      },
      "source": [
        "**Points**: $0.0$ of $1.5$\n",
        "**Comments**: None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxNvAwvwy_ks",
        "colab_type": "text"
      },
      "source": [
        "**Task 13. Training using the best hyperparameters** [0.5 point]\n",
        "\n",
        "Complete the code below to select the best hyperparamater combination and then fit the training data using the selected learning rate and number of epochs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GR2U4pEzP17",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ebd5a48-4fe9-4148-d790-8515ff7add1e"
      },
      "source": [
        "# hyperparameters combinations \n",
        "\n",
        "np.random.seed(0) #to reproduce the results \n",
        "learning_rates = [0.01, 0.1, 1] \n",
        "epochs = [25, 50, 100]\n",
        "# TODO : use the function defined in task 12 to find the best hyperparameter combination from the above list\n",
        "best_hyperparams , best_loss= select_best_hyperparams(train_dataset,learning_rates, epochs)\n",
        "\n",
        "print('Best hyperparameters using validation data.\\nLearning rate: %5.3f, Number of epochs: %d '% (best_hyperparams[1], best_hyperparams[0]))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 net_loss: 13.891\n",
            "epoch: 1 net_loss: 12.061\n",
            "epoch: 2 net_loss: 10.455\n",
            "epoch: 3 net_loss:  9.307\n",
            "epoch: 4 net_loss:  8.513\n",
            "epoch: 5 net_loss:  7.938\n",
            "epoch: 6 net_loss:  7.515\n",
            "epoch: 7 net_loss:  7.228\n",
            "epoch: 8 net_loss:  6.942\n",
            "epoch: 9 net_loss:  6.712\n",
            "epoch: 10 net_loss:  6.520\n",
            "epoch: 11 net_loss:  6.374\n",
            "epoch: 12 net_loss:  6.267\n",
            "epoch: 13 net_loss:  6.179\n",
            "epoch: 14 net_loss:  6.046\n",
            "epoch: 15 net_loss:  5.965\n",
            "epoch: 16 net_loss:  5.880\n",
            "epoch: 17 net_loss:  5.787\n",
            "epoch: 18 net_loss:  5.696\n",
            "epoch: 19 net_loss:  5.619\n",
            "epoch: 20 net_loss:  5.544\n",
            "epoch: 21 net_loss:  5.474\n",
            "epoch: 22 net_loss:  5.411\n",
            "epoch: 23 net_loss:  5.329\n",
            "epoch: 24 net_loss:  5.241\n",
            "epoch: 0 net_loss:  9.941\n",
            "epoch: 1 net_loss:  6.750\n",
            "epoch: 2 net_loss:  6.196\n",
            "epoch: 3 net_loss:  5.875\n",
            "epoch: 4 net_loss:  5.393\n",
            "epoch: 5 net_loss:  5.137\n",
            "epoch: 6 net_loss:  4.828\n",
            "epoch: 7 net_loss:  4.480\n",
            "epoch: 8 net_loss:  4.258\n",
            "epoch: 9 net_loss:  4.032\n",
            "epoch: 10 net_loss:  3.892\n",
            "epoch: 11 net_loss:  3.696\n",
            "epoch: 12 net_loss:  3.524\n",
            "epoch: 13 net_loss:  3.377\n",
            "epoch: 14 net_loss:  3.294\n",
            "epoch: 15 net_loss:  3.199\n",
            "epoch: 16 net_loss:  3.088\n",
            "epoch: 17 net_loss:  2.995\n",
            "epoch: 18 net_loss:  2.925\n",
            "epoch: 19 net_loss:  2.841\n",
            "epoch: 20 net_loss:  2.821\n",
            "epoch: 21 net_loss:  2.674\n",
            "epoch: 22 net_loss:  2.661\n",
            "epoch: 23 net_loss:  2.618\n",
            "epoch: 24 net_loss:  2.559\n",
            "epoch: 25 net_loss:  2.473\n",
            "epoch: 26 net_loss:  2.397\n",
            "epoch: 27 net_loss:  2.447\n",
            "epoch: 28 net_loss:  2.434\n",
            "epoch: 29 net_loss:  2.386\n",
            "epoch: 30 net_loss:  2.354\n",
            "epoch: 31 net_loss:  2.365\n",
            "epoch: 32 net_loss:  2.365\n",
            "epoch: 33 net_loss:  2.377\n",
            "epoch: 34 net_loss:  2.301\n",
            "epoch: 35 net_loss:  2.310\n",
            "epoch: 36 net_loss:  2.239\n",
            "epoch: 37 net_loss:  2.243\n",
            "epoch: 38 net_loss:  2.190\n",
            "epoch: 39 net_loss:  2.132\n",
            "epoch: 40 net_loss:  2.208\n",
            "epoch: 41 net_loss:  2.192\n",
            "epoch: 42 net_loss:  2.122\n",
            "epoch: 43 net_loss:  2.177\n",
            "epoch: 44 net_loss:  2.098\n",
            "epoch: 45 net_loss:  2.092\n",
            "epoch: 46 net_loss:  2.026\n",
            "epoch: 47 net_loss:  2.032\n",
            "epoch: 48 net_loss:  2.057\n",
            "epoch: 49 net_loss:  1.983\n",
            "epoch: 0 net_loss:  8.491\n",
            "epoch: 1 net_loss:  7.000\n",
            "epoch: 2 net_loss:  6.947\n",
            "epoch: 3 net_loss:  6.529\n",
            "epoch: 4 net_loss:  6.373\n",
            "epoch: 5 net_loss:  6.140\n",
            "epoch: 6 net_loss:  5.822\n",
            "epoch: 7 net_loss:  5.949\n",
            "epoch: 8 net_loss:  5.776\n",
            "epoch: 9 net_loss:  5.809\n",
            "epoch: 10 net_loss:  5.871\n",
            "epoch: 11 net_loss:  5.848\n",
            "epoch: 12 net_loss:  5.587\n",
            "epoch: 13 net_loss:  5.608\n",
            "epoch: 14 net_loss:  5.736\n",
            "epoch: 15 net_loss:  5.414\n",
            "epoch: 16 net_loss:  5.331\n",
            "epoch: 17 net_loss:  5.290\n",
            "epoch: 18 net_loss:  5.377\n",
            "epoch: 19 net_loss:  5.181\n",
            "epoch: 20 net_loss:  5.355\n",
            "epoch: 21 net_loss:  5.260\n",
            "epoch: 22 net_loss:  5.138\n",
            "epoch: 23 net_loss:  5.319\n",
            "epoch: 24 net_loss:  5.141\n",
            "epoch: 25 net_loss:  5.022\n",
            "epoch: 26 net_loss:  5.161\n",
            "epoch: 27 net_loss:  5.255\n",
            "epoch: 28 net_loss:  5.156\n",
            "epoch: 29 net_loss:  5.159\n",
            "epoch: 30 net_loss:  4.988\n",
            "epoch: 31 net_loss:  4.765\n",
            "epoch: 32 net_loss:  5.110\n",
            "epoch: 33 net_loss:  4.892\n",
            "epoch: 34 net_loss:  4.739\n",
            "epoch: 35 net_loss:  4.870\n",
            "epoch: 36 net_loss:  4.739\n",
            "epoch: 37 net_loss:  4.979\n",
            "epoch: 38 net_loss:  4.830\n",
            "epoch: 39 net_loss:  4.836\n",
            "epoch: 40 net_loss:  4.702\n",
            "epoch: 41 net_loss:  4.504\n",
            "epoch: 42 net_loss:  4.687\n",
            "epoch: 43 net_loss:  4.844\n",
            "epoch: 44 net_loss:  4.548\n",
            "epoch: 45 net_loss:  4.767\n",
            "epoch: 46 net_loss:  4.755\n",
            "epoch: 47 net_loss:  4.724\n",
            "epoch: 48 net_loss:  4.555\n",
            "epoch: 49 net_loss:  4.651\n",
            "epoch: 50 net_loss:  4.524\n",
            "epoch: 51 net_loss:  4.358\n",
            "epoch: 52 net_loss:  4.425\n",
            "epoch: 53 net_loss:  4.421\n",
            "epoch: 54 net_loss:  4.469\n",
            "epoch: 55 net_loss:  4.246\n",
            "epoch: 56 net_loss:  4.225\n",
            "epoch: 57 net_loss:  4.371\n",
            "epoch: 58 net_loss:  4.336\n",
            "epoch: 59 net_loss:  4.439\n",
            "epoch: 60 net_loss:  4.316\n",
            "epoch: 61 net_loss:  4.525\n",
            "epoch: 62 net_loss:  4.378\n",
            "epoch: 63 net_loss:  4.289\n",
            "epoch: 64 net_loss:  4.547\n",
            "epoch: 65 net_loss:  4.415\n",
            "epoch: 66 net_loss:  4.230\n",
            "epoch: 67 net_loss:  4.429\n",
            "epoch: 68 net_loss:  4.246\n",
            "epoch: 69 net_loss:  4.481\n",
            "epoch: 70 net_loss:  4.408\n",
            "epoch: 71 net_loss:  4.304\n",
            "epoch: 72 net_loss:  4.330\n",
            "epoch: 73 net_loss:  4.218\n",
            "epoch: 74 net_loss:  4.138\n",
            "epoch: 75 net_loss:  4.164\n",
            "epoch: 76 net_loss:  4.318\n",
            "epoch: 77 net_loss:  4.232\n",
            "epoch: 78 net_loss:  4.315\n",
            "epoch: 79 net_loss:  4.192\n",
            "epoch: 80 net_loss:  4.221\n",
            "epoch: 81 net_loss:  4.171\n",
            "epoch: 82 net_loss:  4.238\n",
            "epoch: 83 net_loss:  4.265\n",
            "epoch: 84 net_loss:  4.058\n",
            "epoch: 85 net_loss:  4.209\n",
            "epoch: 86 net_loss:  4.118\n",
            "epoch: 87 net_loss:  4.009\n",
            "epoch: 88 net_loss:  4.144\n",
            "epoch: 89 net_loss:  4.028\n",
            "epoch: 90 net_loss:  4.174\n",
            "epoch: 91 net_loss:  4.250\n",
            "epoch: 92 net_loss:  4.254\n",
            "epoch: 93 net_loss:  4.060\n",
            "epoch: 94 net_loss:  3.979\n",
            "epoch: 95 net_loss:  4.071\n",
            "epoch: 96 net_loss:  4.136\n",
            "epoch: 97 net_loss:  3.997\n",
            "epoch: 98 net_loss:  4.150\n",
            "epoch: 99 net_loss:  3.950\n",
            "Best hyperparameters using validation data.\n",
            "Learning rate: 0.100, Number of epochs: 50 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hctxrQoYzrJX",
        "colab_type": "text"
      },
      "source": [
        "**Points**: $0.0$ of $0.5$\n",
        "**Comments**: None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBAd61cVhce8",
        "colab_type": "text"
      },
      "source": [
        "**Task 14. Logistic regression threshold** \n",
        "\n",
        "Logistic regression takes an input and returns a values between 0 and 1. To interpret this output as a probability of the input being in a class, we need to define a threshold. We set a threshold of 0.5.\n",
        "\n",
        "We predict class 0 if f(x) is greater than or equal to 0.5, else we predict the data point to be of an instance of class 1.\n",
        "\n",
        "Complete the following function which predicts the class (ship or car) of a given input. [1 point]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgfTLnN9hrZR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "outputId": "28509f5a-2cc4-4170-c34b-79113269b25e"
      },
      "source": [
        "threshold = 0.5\n",
        "l_Rate, epoch = best_hyperparams[1], best_hyperparams[0]\n",
        "weight, loss = fit(train_dataset,epoch, l_Rate)\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 net_loss:  9.312\n",
            "epoch: 1 net_loss:  6.980\n",
            "epoch: 2 net_loss:  6.266\n",
            "epoch: 3 net_loss:  5.766\n",
            "epoch: 4 net_loss:  5.301\n",
            "epoch: 5 net_loss:  4.920\n",
            "epoch: 6 net_loss:  4.584\n",
            "epoch: 7 net_loss:  4.222\n",
            "epoch: 8 net_loss:  3.962\n",
            "epoch: 9 net_loss:  3.722\n",
            "epoch: 10 net_loss:  3.538\n",
            "epoch: 11 net_loss:  3.402\n",
            "epoch: 12 net_loss:  3.342\n",
            "epoch: 13 net_loss:  3.225\n",
            "epoch: 14 net_loss:  3.164\n",
            "epoch: 15 net_loss:  3.101\n",
            "epoch: 16 net_loss:  3.075\n",
            "epoch: 17 net_loss:  2.973\n",
            "epoch: 18 net_loss:  2.890\n",
            "epoch: 19 net_loss:  2.840\n",
            "epoch: 20 net_loss:  2.688\n",
            "epoch: 21 net_loss:  2.758\n",
            "epoch: 22 net_loss:  2.722\n",
            "epoch: 23 net_loss:  2.646\n",
            "epoch: 24 net_loss:  2.590\n",
            "epoch: 25 net_loss:  2.545\n",
            "epoch: 26 net_loss:  2.502\n",
            "epoch: 27 net_loss:  2.455\n",
            "epoch: 28 net_loss:  2.435\n",
            "epoch: 29 net_loss:  2.421\n",
            "epoch: 30 net_loss:  2.340\n",
            "epoch: 31 net_loss:  2.344\n",
            "epoch: 32 net_loss:  2.288\n",
            "epoch: 33 net_loss:  2.274\n",
            "epoch: 34 net_loss:  2.290\n",
            "epoch: 35 net_loss:  2.260\n",
            "epoch: 36 net_loss:  2.248\n",
            "epoch: 37 net_loss:  2.180\n",
            "epoch: 38 net_loss:  2.170\n",
            "epoch: 39 net_loss:  2.148\n",
            "epoch: 40 net_loss:  2.160\n",
            "epoch: 41 net_loss:  2.158\n",
            "epoch: 42 net_loss:  2.134\n",
            "epoch: 43 net_loss:  2.105\n",
            "epoch: 44 net_loss:  2.047\n",
            "epoch: 45 net_loss:  2.083\n",
            "epoch: 46 net_loss:  2.019\n",
            "epoch: 47 net_loss:  2.112\n",
            "epoch: 48 net_loss:  2.077\n",
            "epoch: 49 net_loss:  2.063\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFD9TuHPGSc3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(x):\n",
        "  # TODO : compute the predicted class label\n",
        "    z = torch.mm(weight, x.t())\n",
        "\n",
        "    z = sigmoid(z)\n",
        "    # fc = nn.Linear(1024, 1)\n",
        "    # z = fc(z).view(1)\n",
        "    return z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2BYgaQ8zc8m",
        "colab_type": "text"
      },
      "source": [
        "[link text](https://)**Points**: $0.0$ of $1$\n",
        "**Comments**: None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDVYSavWzRJu",
        "colab_type": "text"
      },
      "source": [
        "**Task 15. Reporting accuracy on test set** [*0.5* point]\n",
        "\n",
        "The test set is used to give an indication of the generalization abilities of the model, that is to estimate how good the model is over random guessing at an unseen data point.\n",
        "\n",
        "Complete the code below to compute the accuracy of logistic regression model on the test set. For this, first bring the test set to the low dimensional subspace and then make predictions using the trained model. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFhixtHzhTmM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3e403d0c-79c7-4f81-b423-9e58792718ec"
      },
      "source": [
        "# TODO : bring the test set into the low dimesnional subspace defined earlier for the train set\n",
        "reduced_test_set = None\n",
        "\n",
        "# TODO: compute the accuracy on the reduced test set\n",
        "acc = 0.0\n",
        "n = 0\n",
        "for image, label in test_dataset:\n",
        "    n += label.shape[0] \n",
        "    image = image.view(image.size()[0],3*32*32)\n",
        "    out = predict(image)\n",
        "\n",
        "    out = (out>0.5)\n",
        "    correct = (out == label).float().sum()\n",
        "    acc+= correct\n",
        "print(n)\n",
        "acc = acc/n\n",
        "\n",
        "\n",
        "\n",
        "print('Accuracy on the test set : %6.3f'% (acc))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n",
            "Accuracy on the test set :  0.754\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8ijS9_rntvo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c08fa789-3d2e-454c-8f95-12c101b63cd7"
      },
      "source": [
        "print(1==True)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua7waeyzzDzP",
        "colab_type": "text"
      },
      "source": [
        "**Points**: $0.0$ of $0.5$\n",
        "**Comments**: None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bXF9_CIGT3D",
        "colab_type": "text"
      },
      "source": [
        "**Task 16. Improving accuracy on test set** [*5* point]\n",
        "\n",
        "Use pytorch's neural network layer functions and construct a model which gives better accuracies for the same training and test set. You can have a look at the torch.nn package for this. \n",
        "\n",
        "Describe the model and explain why it performs better ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hBKmfSRVZ0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_model():\n",
        "  # TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OKJfe-AHXb9",
        "colab_type": "text"
      },
      "source": [
        "**Points**: $0.0$ of $5$\n",
        "**Comments**: None"
      ]
    }
  ]
}